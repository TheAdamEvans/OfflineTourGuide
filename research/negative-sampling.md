# Negative Sampling Strategy

For each manifest row:

Generate:

1Ã— N (gold, your existing spec)

1Ã— S (bland, generic, rule-breaking brochure)

1Ã— E (logistics-heavy, story-light)

1Ã— W (story-heavy, logistics-light)

Optional 2â€“4Ã— trait-ablations by editing N (no sensory, no indigenous, etc.)

Run them through the big model, capture activations.


Compute:

N_dir = mean(N âˆ’ S)

EW_dir = mean(E âˆ’ W)

Trait_dirs = mean(N âˆ’ N_without_trait) for each trait


Use those:

To steer the small model (adding/subtracting scaled directions)

To filter / project out directions associated with non-essential traits.

## More Insights

### Be very intentional about what you align

Instead of â€œalign the whole hidden stateâ€, align the subspace thatâ€™s actually about this task.

a. Build task-specific subspaces first

From your N / S / E / W and trait ablations:

For each pair (same prompt, different style):


Student: same but with student activations.

PCA / SVD to get the main tour-related subspace in the teacher.

Only align these low-dimensional subspaces (e.g. top 32â€“128 components), not the entire residual stream.

This helps you avoid:

Rotating huge chunks of space that have nothing to do with tours.

Overfitting to noise in a tiny dataset.

This matches how people use Procrustes to align embedding spaces across models / time periods.



Use weighted Procrustes / SVD

Procrustes is usually just minimize over orthogonal ğ‘… but youâ€™ve got richer structure:

Some examples are positive (N, trait-preserving)

Some are negative (S, ablations)


You can:

Form a single stacked matrix of paired directions

Rows = [Nâˆ’S, Eâˆ’W, trait diffs, etc.]

Assign weights:

High weight to:

Nâˆ’S (core â€œgood vs bad tourâ€)

Lower weight to:

Eâˆ’W (if you want to preserve both â€œlogisticsâ€ and â€œstoryâ€ as a controllable axis rather than fully aligning)

Solve a weighted Procrustes problem or, simpler, rescale rows of A and B before computing SVD.

This bakes your â€œwhat matters mostâ€ into the rotation itself instead of treating all pairs equally.



## FAQ

Why not generate JSON, or assembly? And use that as a negative example?

â€œNearâ€ vs â€œfarâ€ negatives
What we designed above are near-manifold negatives:
Same task (â€œbe a tour guide for this locationâ€)
Same format (short narrative blurb)
Only one or two capabilities flipped (no anecdotes, no sensory, etc.)
Those are gold for:
Getting clean Nâˆ’S directions that correspond to tour-guide-ness, not â€œcompletely different activityâ€.
Letting you say: â€œthis neuron activity is about sensory detail, that one about logistics, etc.â€

Now compare that with:
JSON response
Assembly code
Some tool-spec format, etc.

Those are far-manifold negatives:
Different discourse structure
Different token distribution
Often a different task entirely (specification / code-gen instead of narrative guidance)
Those will give you very strong directions like:
â€œNarrative prose vs structured machine-readable outputâ€
â€œNatural language vs codeâ€


â€¦but they wonâ€™t tell you much about:
Good vs bad tour guidance
Rich vs bland sensory description
Authentic vs tourist-trappy recs
So for your tour-guide steering, JSON/asm wonâ€™t replace the structured N/S/E/W we talked about.



Is there anything like this out there already?

I havenâ€™t seen an activation-steering paper whose primary contribution is:

â€œUse ActAdd-style directions to compute an orthogonal transform, then reparameterize the Studentâ€™s weights once so there is zero inference overhead.â€

Closest neighbors I see are about:

change-of-basis schemes for efficiency inside a single model (not for alignment / distillation),

or standard feature distillation with learned losses, not closed-form Procrustes.


Youâ€™re doing something more structured:

For a specific task (tour guide), you design:

N/S (great tour vs deliberately-bad brochure)

E/W (logistics-heavy vs story-heavy)

plus trait-specific ablations (sensory vs no-sensory, indigenous vs not, etc.).

You explicitly treat these as orthogonal-ish axes to define a task subspace in the Teacher.

Then you align the Studentâ€™s corresponding subspace to that Teacher subspace via Procrustes, using those carefully tuned contrasts.

Thatâ€™s a tighter, more â€œgeometric distillationâ€ view than ActAdd, which is mostly â€œwe found a vector; adding it helpsâ€.