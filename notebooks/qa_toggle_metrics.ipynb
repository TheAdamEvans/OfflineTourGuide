{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QA Toggle + Metrics Skeleton\n",
        "\n",
        "Use this notebook to validate config-driven evaluation toggles. It accepts a config file (JSON/TOML), enumerates checkpoints, and writes placeholder metric rows (logit KL, hidden-state cosine, residual RMS) to the configured JSONL output. Real metrics can slot into the stub helpers later without changing the surrounding plumbing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from config import EvalConfig, load_eval_config\n",
        "\n",
        "CONFIG_PATH = Path(\n",
        "    os.environ.get(\n",
        "        \"OFFLINE_TOUR_GUIDE_EVAL_CONFIG\",\n",
        "        \"config/eval_template.json\",\n",
        "    )\n",
        ")\n",
        "config: EvalConfig = load_eval_config(CONFIG_PATH if CONFIG_PATH.exists() else None)\n",
        "config.ensure_output_dirs()\n",
        "\n",
        "CHECKPOINTS = config.checkpoints or [\n",
        "    f\"runs/{config.run_id}/checkpoints/base.pt\",\n",
        "    f\"runs/{config.run_id}/checkpoints/permutations_only.pt\",\n",
        "    f\"runs/{config.run_id}/checkpoints/permutations_rotations.pt\",\n",
        "    f\"runs/{config.run_id}/checkpoints/permutations_rotations_style.pt\",\n",
        "]\n",
        "print(f\"Using config at: {CONFIG_PATH.resolve() if CONFIG_PATH.exists() else 'defaults'}\")\n",
        "print(f\"Writing metrics to: {config.metrics_output_path}\")\n",
        "print(f\"Checkpoints ({len(CHECKPOINTS)}):\")\n",
        "for path in CHECKPOINTS:\n",
        "    print(f\" • {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _stub_metrics(metric_flags: Dict[str, bool], idx: int) -> Dict[str, float | None]:\n",
        "    rng = np.random.default_rng(seed=idx)\n",
        "    stub: Dict[str, float | None] = {}\n",
        "    if metric_flags.get(\"logit_kl\", False):\n",
        "        stub[\"logit_kl\"] = round(float(rng.uniform(0.05, 0.4)), 4)\n",
        "    if metric_flags.get(\"hidden_state_cosine\", False):\n",
        "        stub[\"hidden_state_cosine\"] = round(float(rng.uniform(0.6, 0.95)), 4)\n",
        "    if metric_flags.get(\"residual_rms\", False):\n",
        "        stub[\"residual_rms\"] = round(float(rng.uniform(0.8, 1.1)), 4)\n",
        "    if metric_flags.get(\"surprisal\", False):\n",
        "        stub[\"surprisal\"] = round(float(rng.uniform(15.0, 35.0)), 2)\n",
        "    return stub\n",
        "\n",
        "\n",
        "def evaluate_checkpoint_stub(checkpoint_path: str, config: EvalConfig, idx: int) -> Dict[str, Any]:\n",
        "    metric_flags = config.metrics.to_dict()\n",
        "    variant_flags = config.variant.to_dict()\n",
        "    payload: Dict[str, Any] = {\n",
        "        \"run_id\": config.run_id,\n",
        "        \"checkpoint\": checkpoint_path,\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"metric_flags\": metric_flags,\n",
        "        \"variant_flags\": variant_flags,\n",
        "        \"metrics\": _stub_metrics(metric_flags, idx),\n",
        "        \"notes\": config.notes or \"stub metrics pending transport eval\",\n",
        "    }\n",
        "    return payload\n",
        "\n",
        "\n",
        "def write_metrics(rows: list[Dict[str, Any]], destination: Path) -> None:\n",
        "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with destination.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "        for row in rows:\n",
        "            handle.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = [evaluate_checkpoint_stub(path, config, idx) for idx, path in enumerate(CHECKPOINTS)]\n",
        "write_metrics(rows, config.metrics_path)\n",
        "print(f\"Wrote {len(rows)} metric rows → {config.metrics_path}\")\n",
        "rows[-1]\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
